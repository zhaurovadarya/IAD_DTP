from mlxtend.frequent_patterns import apriori, association_rules
from tqdm import tqdm
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("df_rules.csv", low_memory=False)
razmet_df = pd.read_csv("razmetPF.csv")
print(f"df_rules: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
print(f"–†–∞–∑–º–µ—Ç–∫–∞ –ü–î–î: {razmet_df.shape[0]} –ø—É–Ω–∫—Ç–æ–≤")

exclude_cols = ['latitude', 'longitude', 'severity_score', 'fatality_count', 'injury_count'] #–±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
features = [col for col in df.columns if col not in exclude_cols and col != 'severity_class']

df_bin = df[features].fillna(0).astype(bool)
severity_class = df['severity_class'].fillna('unknown').astype(str) # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∏–Ω severity_class
severity_bin = pd.get_dummies(severity_class, prefix='severity_class')
df_bin = pd.concat([df_bin, severity_bin], axis=1)
frequent_itemsets = apriori(df_bin, min_support=0.01, use_colnames=True)
print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(frequent_itemsets)} —á–∞—Å—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0) # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª
rules = rules[rules['consequents'].apply(lambda x: any('severity_class' in item for item in x))] # –ø—Ä–∞–≤–∏–ª–∞, –≥–¥–µ consequents —Å–≤—è–∑–∞–Ω—ã —Å severity_class
rules = rules.sort_values(by="lift", ascending=False).reset_index(drop=True)
print(f"üîπ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∞–≤–∏–ª: {len(rules)}")

# –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ antecedents –∏ consequents –≤ —Å–ø–∏—Å–∫–∏
rules['antecedents_list'] = rules['antecedents'].apply(lambda x: list(x))
rules['consequents_list'] = rules['consequents'].apply(lambda x: list(x))
proposals = [] # —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º / –ª–µ–º–º–∞–º
for idx, rule in tqdm(rules.iterrows(), total=len(rules), desc="–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª"):
    antecedents = rule['antecedents_list']

    for _, pdd_row in razmet_df.iterrows():
        related_factors = str(pdd_row['related_factors']).split(', ')
        matched_factors = []

        for a in antecedents:
            for rf in related_factors:
                if a.lower() in rf.lower() or rf.lower() in a.lower():
                    matched_factors.append(rf)

        if matched_factors:
            proposals.append({
                "rule_index": idx,
                "antecedents": rule['antecedents'],
                "consequents": rule['consequents'],
                "lift": rule['lift'],
                "pdd_id": pdd_row['pdd_id'],
                "pdd_text": pdd_row['pdd_text'],
                "themes": pdd_row['themes'],
                "matched_factors": ", ".join(set(matched_factors))
            })

proposals_df = pd.DataFrame(proposals)
proposals_df.to_csv("PDD_coincidence.csv", index=False, encoding="utf-8-sig")
print(f"üíæ –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –ü–î–î –≤—ã–ø–æ–ª–Ω–µ–Ω–æ, {len(proposals_df)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

# —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
mean_support = rules['support'].mean()
mean_confidence = rules['confidence'].mean()
mean_lift = rules['lift'].mean()
print(f"–°—Ä–µ–¥–Ω–∏–π Support: {mean_support:.3f}")
print(f"–°—Ä–µ–¥–Ω–∏–π Confidence: {mean_confidence:.3f}")
print(f"–°—Ä–µ–¥–Ω–∏–π Lift: {mean_lift:.3f}")

rules.to_csv("assocPR.csv", index=False, encoding="utf-8-sig")
print("üíæ –ê—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ assocPR.csv")

# –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
factor_counts = proposals_df['matched_factors'].str.split(', ').explode().value_counts()
top_factors = factor_counts.head(10).index.tolist()
pdd_counts = proposals_df.groupby('pdd_id').size().sort_values(ascending=False)
top_pdds = pdd_counts.head(10).index.tolist()

subset = proposals_df[ # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    proposals_df['matched_factors'].str.split(', ').apply(
        lambda x: any(f in top_factors for f in x))]
subset = subset[subset['pdd_id'].isin(top_pdds)]

matrix = pd.DataFrame(0.0, index=top_pdds, columns=top_factors)
pair_counts = {(p, f): 0 for p in top_pdds for f in top_factors}

for _, row in subset.iterrows():
    pdd = row['pdd_id']
    factors = row['matched_factors'].split(', ')
    for f in factors:
        if f in top_factors:
            pair_counts[(pdd, f)] += 1
pdd_total = {pdd: 0 for pdd in top_pdds}
for (pdd, f), count in pair_counts.items():
    pdd_total[pdd] += count
for (pdd, f), count in pair_counts.items():
    if pdd_total[pdd] > 0:
        matrix.loc[pdd, f] = count / pdd_total[pdd]
    else:
        matrix.loc[pdd, f] = 0.0
plt.figure(figsize=(14, 7))
sns.heatmap(matrix, annot=True, fmt=".2f", cmap='YlGnBu')
plt.xlabel("–§–∞–∫—Ç–æ—Ä—ã", fontsize=14)
plt.ylabel("–ü—É–Ω–∫—Ç—ã –ü–î–î", fontsize=14)
plt.title("–°–∏–ª–∞ —Å–≤—è–∑–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —Å –ø—É–Ω–∫—Ç–∞–º–∏ –ü–î–î", fontsize=14)
plt.tight_layout()
plt.show()

theme_counts = proposals_df['themes'].str.split(', ').explode().value_counts() #
plt.figure(figsize=(14, 6))
theme_counts.head(20).plot(kind='bar', color='lightgreen')
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π", fontsize=14)
plt.xlabel("–¢–µ–º—ã", fontsize=14)
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–µ–º–∞–º", fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

